====
LOGS
====

params = list(objective = "poisson",
              metric = "rmse",
              seed = 20,
              learning_rate = 0.075,
              lambda = 0.1,
              bagging_fraction = 0.66,
              bagging_freq = 1, 
              colsample_bytree = 0.77,
              num_leaves = 128,
              min_data_in_leaf = 20,
              force_row_wise = TRUE,
              nthread = 7)

m_lgb <- lgb.train(params = params,
                   data = train,
                   nrounds = 4000, # More rounds needed
                   valids = list(valid = valid),
                   early_stopping_rounds = 400,
                   eval_freq = 200)

[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.815358 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 10138
[LightGBM] [Info] Number of data points in the train set: 20213335, number of used features: 88
[LightGBM] [Info] Start training from score 0.230747
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[1]:	valid's rmse:3.429 
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[201]:	valid's rmse:0.11891 
[401]:	valid's rmse:0.11376 
[601]:	valid's rmse:0.114237 
[801]:	valid's rmse:0.113846 


-------------------------------------------------

p <- list(objective = "poisson",
          metric ="rmse",
          seed = 33,
          force_row_wise = TRUE,
          learning_rate = 0.075,
          sub_feature = 0.8,
          sub_row = 0.75,
          bagging_freq = 1,
          lambda_l2 = 0.1,
          nthread = 7)
          
m_lgb <- lgb.train(params = p,
                   data = train,
                   nrounds = 4000, # More rounds needed
                   valids = list(valid = valid),
                   early_stopping_rounds = 400,
                   eval_freq = 200)

[LightGBM] [Info] Total Bins 10138
[LightGBM] [Info] Number of data points in the train set: 20213335, number of used features: 88
[LightGBM] [Info] Start training from score 0.230747
[1]:	valid's rmse:3.34626 
[201]:	valid's rmse:0.119511 
[401]:	valid's rmse:0.0873389 
[601]:	valid's rmse:0.0842484 
[801]:	valid's rmse:0.083714 